{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncuXJJJD1jWZ"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWq_qcRfEJRP"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gpi4LA4v1ojl"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcUl6N6p9dhF"
   },
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692,
     "referenced_widgets": [
      "e9d5aa8b82a9462e9d7e08f94663ca5f",
      "b9dcedb6943f43e6af9ce6030de4ab62",
      "e9034ecb663148f59d72bb70166bd8c9",
      "47412d6982a24efeb76767182a3f7037",
      "50ad5b9d621d42c09facecd09c31fba8",
      "2fa6b9e5d8bb4a4a9e6a265b0f11e594",
      "e8767365efd747d5b57192d7837cf31f",
      "7e7e0672a31843c0b1029eb424a15955",
      "bd6221a82f5b4d6b92208ce74fcaa3ec",
      "f0ca0e2409b443f8afdba0045f07d427",
      "1ab3e7221e0b46838efdd19a77f365aa",
      "83e681d41bc9446f92a77208ec85e124",
      "7af4f08e962644c8873ed42139c1377b",
      "4f2363a268484a4d897d8646945d7eec",
      "8892d427694145ca82e08f529bc8600e",
      "64d6f6a9bc274f54abf0ee848dcead84",
      "1827ef70cac74f3e831c8788a42199dd",
      "1967128bdd1041f09d32bd392388da70",
      "db82a30f0308490c9c30a82e93e3ac82",
      "c2ed69a0babf40e990bb80aa18770dee",
      "76cb8f95740d4c6b831b6efb48eeb20c",
      "98898fe31cf14f23ab587b16809c0390",
      "1415bcdb1c6744d798b60dc92f9d0f7a",
      "d1f7bbf8faeb475b92125a2f81f68e87",
      "fcb124187c2744288a74f1973085a1f1",
      "26f15151a1e342a8a133fc4d0d9acb08",
      "a57dbe826e014e148734de3748cfd996",
      "d49574f2dd1b4a1295f25eb8ca23ac0c",
      "570f65a30e6046c2bc40ad7d42f4e733",
      "3c38d89ff6e6464f8ae36ecee97d35d7",
      "51b9216133d2456db5aaa8c0c4bb5f39",
      "1a13555c81a64a4994c44b944760cb76",
      "f20f28dd13784a71af5cd43c166cad9b",
      "92f19b3369b5497ea6c9ffdd1e630d6a",
      "4d074b7664854a269803102f9c71db81",
      "cdcdd983ffe14dbb8724957ceb17adbb",
      "883c15cc0ec945eeb6f14b2af34c7e91",
      "566eee1c07e743d5940052a648668166",
      "53f09ea19e2f44d0b72ad5725c0a4877",
      "a56b7447d33848de8fa49149985d5205",
      "747b315844cb411b92a12d873bd9cceb",
      "455ac515b83b461c87409b3220bd9c29",
      "775310dda7d442699c35e979d3b7b3e3",
      "4c6d29a0b08b40a3860d03d0cbb10e84",
      "6521d11cfa044af4a54876fe5510976a",
      "2087b835da2b42849540d07a1c06a682",
      "09dd52c4c8284041881a5d8885cf4e1d",
      "4dc13f03593f4c1c801109f2b1aef498",
      "9e98b282b26a440e8b9299d6c6d77315",
      "772245cecf21418a9789d974c453a377",
      "f8ab739b6c0144569d41e366de01b265",
      "d016b6d64dcf413da48b953fe55611cd",
      "29ed5959a03b4994abaa0041e5196fb2",
      "0850dc0e391d4dccb0b85635e72629af",
      "5c821d30a3964d979fbdc3b9bb085725",
      "1d71efa4be2142669018e7b9ceb75142",
      "38ad17704351404e87e39e76f7fa7dfc",
      "d80533c92ae2411d946fcd7913b7f9df",
      "7132b7ffb278468cb6de33f972e73402",
      "57f3da93e29947ebbcea3a38e7592caa",
      "8581479d26944dd08c69c4d0fc823ad9",
      "c93c9d44e4b44271b9284dec34b6b596",
      "b3a3a3286b1540ba98ff1075e30cd35e",
      "62d0ee81209244598c0b1e79f2e7ac44",
      "163f34d1a82f42c3b3d9ec508c1b0711",
      "c603c7eb0cc942dc8e455635bcdfc234",
      "edc11bb06b244a978f30845a63694493",
      "b43db76744a44bbaa66324ab579820f6",
      "7a01860f326a4f1d9b11104adbcd1937",
      "6095e7c3cbf847e4ac158b1bd194da24",
      "9300d2f460784cb194d22868255c1b9c",
      "20afc4b27cf14293851b080f9c76989b",
      "6953724adfb7410fbb1959d6eea875d8",
      "2cbd043750f84c11bad97d12ca5d4633",
      "3df4f7cf87ca468886083775ff57c15a",
      "fc2db54e88e241a580f29df0b168b38c",
      "951d800a4e7b453fa52cef3b30d1f87b"
     ]
    },
    "id": "YJJiRHnn1txO",
    "outputId": "3fb42317-f27d-4261-f44c-a89983fd35a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d5aa8b82a9462e9d7e08f94663ca5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e681d41bc9446f92a77208ec85e124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1415bcdb1c6744d798b60dc92f9d0f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f19b3369b5497ea6c9ffdd1e630d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6521d11cfa044af4a54876fe5510976a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d71efa4be2142669018e7b9ceb75142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc11bb06b244a978f30845a63694493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veydyBGO9Wsf"
   },
   "source": [
    "# Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "22b4e486a70648e4b005534735c02fb9",
      "8ea80750fbf94fb2ac726a3e4d1cd34e",
      "f8346dadda7c410c8a24203d1ebd2657",
      "57be11b0284c448d84948b917971d25d",
      "eabf61a399654b2e8e799e20640d49fe",
      "f2d246bf5aaa489f9efc0d8619cc6bf1",
      "e263b90c208548cda425b3674f02a0a6",
      "3b1cea646b074572bbd23c6eb573fc68",
      "69e0ebab36564ba781ae00b124ef5943",
      "b1b6904da3a54c5bb89604f40dd5532d",
      "3bc956b0bd5945e6a3776c80177bb136",
      "59e4cdee5dc5414ca7466fa93e19b844",
      "fd450fdb1a1748f5838d611676b14a45",
      "2d1439ad39b24d1fb1b0bfa6b4eeffbc",
      "ce6c6d35b4b442009260276db1856f72",
      "d53aee79bc81431780a71828dce18aaf",
      "7a9727e674ed48a18dc0b6c84e7dc1df",
      "14d3e3d5886646929f9a8c288959feba",
      "5e0c82e591b24031a9da2252528ace90",
      "8b94bdf6cbb34d6d9a834d83a10f4fc2",
      "de1c7ed9c67c4107a3e27232c9e1b151",
      "435e7d7a96694b0eb3141cccffe19c29",
      "7b34e043b7d647de8af0fea84dd241bb",
      "96e676606d8b44a7a54de330166f51b8",
      "bcad77ee283c4aea82bd141ec6312f23",
      "2aca0d05397c4496a3155316d52ec860",
      "e436a1d0a468473fb677f5ec6b1b5407",
      "ac86bf4288ab42a4b0a4285af13ded7e",
      "e3bc79a1f73640eab365b686d2f4b4a2",
      "55d7f58f8d1c486fa83342ad9efb09f1",
      "7f7d28ad43384066bc452b95d8f8a01a",
      "cf7194606edf40caa09a3c98636f26c3",
      "f27b8252d7434a3186810f2ec8d4e696",
      "3b32e76f0cce4527bc769a278ac045eb",
      "fc2f236901694cbb854487431bf21c43",
      "e318b54fe2a446caa73c4df8513a2d21",
      "3a3f349865e54a45a5860fc703b77d6a",
      "c5c9688a91a0431e93ad653efdfd4bf5",
      "647d6760ddba463ba8b89ee8797e7788",
      "091413a2e49846dc85772e68ca3a0db7",
      "91d51d393ea34855be4a6c426eb411e5",
      "99b3eb3e52d94f318769cd6d7c23c487",
      "55dd7f30e4ab4a1a96f526ecb438cbc5",
      "fe424fc23f33428892dd606b1b984126",
      "3b93765baf5c4c779e5580621ed93f3d",
      "e97205465a3040da95e2c64551ba46bf",
      "db713913b1d94fb1bbabd965a1150d0f",
      "e0bb1f960fb54a499041e0710e2226ec",
      "44e32954cdbe4a55b2c496cbd90838cf",
      "49988106dada404cac337e58ae4eb7e9",
      "b0487796452b46e495f019fca6420ddc",
      "2efa281ddc754e6ca234fa24e448450f",
      "4e12fd9aaf6044a4a45e280509c013c5",
      "dd55f9a8f1cb4949bad0500787526edf",
      "0895b4489e2a42b6989ae4028890ad01",
      "135d91059047410991ad575ead483f62",
      "b903277ed2054b109915fee9edb77a79",
      "429df217b88d488eaed689c7334ec1dc",
      "4c6c2077982643ee952a3c25197e8133",
      "3755dcf6d63d4a0b97e9d8a6093d3db6",
      "c393a13d06f24a2cbb5aae81e9809ef1",
      "acaa941a5b764f1bb7c70fe78abd188e",
      "4be86bb67b0b49e78daefd42613f7f9b",
      "f0300390d27e4d7496a054e65d9f8b51",
      "56fd848d2fbc4732b188ac75a2b15b38",
      "126536a2ca1044c79a02b033469ef4d9",
      "c074c43956fe431c9002f80fc1decbce",
      "76d212d4041743e1bf6b8df87019d0ee",
      "4f19001c1f4f427db49d3603e3d66493",
      "04b1c2796c584e21a5a4c67ec9ad457d",
      "faf3e0f6883e466c8bb505a89d5ea5f1",
      "3b21c848acb44292b486efdc2e7b8c2d",
      "30169079d8f1446a9dd02e7c09abf774",
      "b819e81bcbcd490c96ef0e8e43154c16",
      "f2870668498546aeb132ca824c07a385",
      "241432fefbfb447186903cc01dcc0a27",
      "f2b904ef81e648eeac0c0fc54bde035d"
     ]
    },
    "id": "yl2OThcM4eOD",
    "outputId": "f72a0024-2260-4400-dcfc-2c8515cf4b7b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b4e486a70648e4b005534735c02fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e4cdee5dc5414ca7466fa93e19b844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b34e043b7d647de8af0fea84dd241bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b32e76f0cce4527bc769a278ac045eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b93765baf5c4c779e5580621ed93f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135d91059047410991ad575ead483f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c074c43956fe431c9002f80fc1decbce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-sjKeBA9HxQ"
   },
   "source": [
    "# Load the Pre-trained Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRzHiAMC4mY5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-5lbPj69mWc"
   },
   "source": [
    "# Train the Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNeYG-4pCjgD",
    "outputId": "0fe30f3d-ece8-4085-c155-ea24f4e14a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CS4TogBcHVB6"
   },
   "outputs": [],
   "source": [
    "# Define smaller subsets for faster training and evaluation\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "7c46c3c2680a42c4ac007931fe32418d",
      "4fd55d69c8e84f99b91a68042728b5c1",
      "76c25f4cfa2144fb86037a63e2c292f4",
      "188aef719fac4a1197952321a0469e3d",
      "79c9a3350fb04298a834099685672579",
      "fb34c811be3b49f7857f0833bf01db16",
      "e0b3b8e473c54d54811d77c18db001dc",
      "a3068a41d42e469b8ca7c334d0290fb8",
      "d9705cc2aff142a4b1e65882ccc5e851",
      "66c27546ef1e44edae5beca41b101881",
      "97a9a97b4ec94819a1fb595363a440be"
     ]
    },
    "id": "69eN-hWx-Byp",
    "outputId": "1ba77f43-10cd-4319-fbd9-ab0cc5e90a41"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c46c3c2680a42c4ac007931fe32418d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3942543011.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# --- 3. Create Trainer ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m trainer = Trainer(\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0;31m# your AutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate # Import the evaluate library\n",
    "\n",
    "# --- 1. Define Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",            # where to save checkpoints\n",
    "    eval_strategy=\"epoch\",       # evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8, # Corrected argument name\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# --- 2. Define Metric ---\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# --- 3. Create Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,                         # your AutoModelForSequenceClassification\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# --- 4. Start Training ---\n",
    "trainer.train()\n",
    "\n",
    "# --- 5. Evaluate ---\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77ANwnywGz4s"
   },
   "source": [
    "# Evaluate the Sentiment Analysis Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "kkD4U4YxCeMX",
    "outputId": "712c2509-02de-47ec-d05a-02b9c795c407"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3249228000640869, 'eval_runtime': 28.5576, 'eval_samples_per_second': 35.017, 'eval_steps_per_second': 4.377, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDuv62dSG2cw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ce078c7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "a50da0b2",
    "outputId": "e02ab0b6-9138-49c6-c4b6-30bf437b75dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2884790045.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define smaller subsets for faster training and evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msmall_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msmall_test_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Define smaller subsets for faster training and evaluation\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Training configurations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",      # Where to save checkpoints\n",
    "    eval_strategy=\"epoch\", # Evaluate every epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\" # Disable logging to Weights & Biases\n",
    ")\n",
    "\n",
    "# Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "b9cdee2c",
    "outputId": "29bf127c-fff3-4937-b13d-8daf7fd880fb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3153324668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenized_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "2354d4c4",
    "outputId": "2fac8f12-5382-474e-8b92-7ddf5cde7ebb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2884790045.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define smaller subsets for faster training and evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msmall_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msmall_test_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Define smaller subsets for faster training and evaluation\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Training configurations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",      # Where to save checkpoints\n",
    "    eval_strategy=\"epoch\", # Evaluate every epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\" # Disable logging to Weights & Biases\n",
    ")\n",
    "\n",
    "# Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "696a68a8",
    "outputId": "219d5fcd-d726-4cf6-fc7a-09fdcd1dd55a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "43ef44578185484e9ca99ab47c37a777",
      "2cdb7264cf884ef9b003f4cfed05bea4",
      "2ba2852bf5384e9486e75be826aa937c",
      "f0c028c84b2349ef8ee53cec71f7610d",
      "657456845e1b473f981d0c4495b870e4",
      "97ab3e8a58fc42a1be26fbdc25e766fa",
      "a32f67104e944aada9b2159e9232fd6f",
      "adaafea7625b4c35be0a3439042f450c",
      "1cd4d9d84e2444aaa6d43465aadad1b3",
      "6200241c7ec540ecb92eec3182db6c23",
      "59dcc4c8fcee4031952f86ac945e74dd",
      "c5e05095ff0a4031951264dc90b570a0",
      "d3ec414976f4430eafa32ff4caff7681",
      "e869e407ac8f4e0d9bee3929c1b76864",
      "165b385754bb47d48a3cd981d28dc347",
      "70fb2d7c0bb44031832ae0569b7a6e3c",
      "8d106a3a4d9644b085a2dd94dbc95fb7",
      "5239f11f955a431d907cf2aa7e2c4eae",
      "6236daafb8ad422c9587680d0efdc107",
      "742d5d19e9b94f98a3c6c4da7e160af3",
      "da422d77de094c3eac358e02db2f76cf",
      "168a6b2887494ca7aa1e2003d18d71cf",
      "c184eb5fcf0a4540b6f152e62d49184b",
      "51e6e3ed73404a55b5c1b067a1aa328d",
      "b99c91fbc3a34f27b8877afee75bffd7",
      "b0c427e388dc4403b4e4b6a108d9662c",
      "65df44e9250e49acba264498da9e4a8a",
      "aa23f647f9e541ccaad49f658d1866f4",
      "8af6eecaeb2e4b18b7213eaae086723d",
      "8fea6b3f5cc9457db8eebcc0e2608491",
      "a61d7977f89e415caf6b5f63475d134b",
      "8fad615a64bd49eb9d4435277dd60f53",
      "c660980edbb74ffe9f925ca9dc8bc923"
     ]
    },
    "id": "926ad41b",
    "outputId": "341bb30f-7347-4364-a325-619e0a421710"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ef44578185484e9ca99ab47c37a777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e05095ff0a4031951264dc90b570a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c184eb5fcf0a4540b6f152e62d49184b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnFMi0PiJSQp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "65f0a2b8",
    "outputId": "cee6bdae-0403-4bd1-89f4-14bf4289ffe6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2884790045.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define smaller subsets for faster training and evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msmall_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msmall_test_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Define smaller subsets for faster training and evaluation\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Training configurations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",      # Where to save checkpoints\n",
    "    eval_strategy=\"epoch\", # Evaluate every epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\" # Disable logging to Weights & Biases\n",
    ")\n",
    "\n",
    "# Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846,
     "referenced_widgets": [
      "6a814d51fb3c416e944c314699a67013",
      "ee613320d4394002956e8d6a8b220d4e",
      "9ddd9b12735146d9baea8f19705d4f5b",
      "5e93cba1de33455d9e61d00a52ad8883",
      "a435509a88bc4134b83925b9e38e492f",
      "cbd42aa5d712446b8eca2b759e84c46c",
      "fe0fa40124e642c4a61109e72b7d32dd",
      "e3bd974a524042548dec7513fc734cef",
      "75a2255434344baf81936d43fae9fd6f",
      "8b8df1ea22064daea5551175d0ba26b7",
      "24e74a19fabe4d96a368d7c5ef663767",
      "a5ca27ccc009460bb8e8bbd7854d66ae",
      "86877da97aec4987a0344f931052f32e",
      "acda52f710be4e27b36cabc6fe475d26",
      "f3520a7ca04d4f0eb6e58e0a05d9f2df",
      "17ae7a83241843e6ac9356dc2caf98b4",
      "0fa2b359510a4160b68cd30dc77fd86b",
      "d0d984527e264a69a47b5752b5af6c0a",
      "4e8ff9cb4d4f4420bdb56daca2f987f5",
      "23a7632e24d24c5cb9046c534e864a56",
      "c09c3b1d83e74208a3c9bf348451616d",
      "96fe468b58ac4d178673dd030195df83",
      "22ce52d9ed8349f39c9d637ca87e812e",
      "546c1a87686e419b89794165bbb081ff",
      "c266ba0e09fb43d1997a758c9c83aa39",
      "1bebb559d5084e168b3960bacea6bbfb",
      "dd25732c10d949359e4444be8b3087a4",
      "914ad9db18474ddb8bfeb848e79f8e01",
      "16fc7c7c0a624076939d88cbcbfac4dc",
      "fd88a08506f34048970ff15737b5caae",
      "37a927c9b0db4f0aad74ddcc2cb466bb",
      "f6188e44c7ae42b3bcae880b17cb5eef",
      "0e22855504ff451aa1a33238ea69efbb",
      "9f17714b31ea42adb0aac19a2619741d",
      "4992df98d9ef4d2cbe5cda280ce342db",
      "b17f632c6e1b4be58634fdf0559a6eaf",
      "92f0e6eb64574104b2b6abbeb2e5b5e1",
      "a132d955abef4d38a88fd5b67176fc20",
      "605b84149c564ae096c747d0abb92dd6",
      "182dce1d129c43ef942d8eb891b6586d",
      "44dd4769778f4dfc8d1c6aada95e74c3",
      "3293a25e82dc4f08a90c3c702228abc6",
      "bfa7b9ba898b4308997d18f2f5359a36",
      "f334651fc20742db85647db096149e6b",
      "1e8200a63fc44cca8828903b86675e60",
      "b8fde4bd1323466a890de5cbe8ed8541",
      "673890ff874244fdb4b9ab820db7f629",
      "f304d1445ea14b3ca90c34cb955dc0f0",
      "cafeb57848c74082aefd4c00ec551795",
      "8617acc4c2c2437c9e4e747a51bffb20",
      "71e3e4b047e44f3a9216100770cb3d2a",
      "b7b4970d1b6241f3b3a94ca1c7a0cb90",
      "e711ed5b81e04fd9b33e899229c9d951",
      "e6deeef2586f4e9abf01380b28208d4a",
      "6076904f25bf48fe9bcaa1632faacf1f",
      "77f6e822315a49cbb12d004e15d0992a",
      "c4ff0d6924114b2891cd40d5e1e76015",
      "385d9c70841a4a67a2ef0599282dd704",
      "e80b592f6031449da1ae38c331847956",
      "4009c4b6c2ba4c90a4ee8a8c2786c512",
      "f96e5101777246cebb2ddb256e81635c",
      "97d6986c221346c5b62364948cf9edd5",
      "9e55b89c2b3941378ec346c1d404b869",
      "4c26550b8aef482db5ae7c0fdaf5635c",
      "e1ffc07ce5d5494cbd08e0bbe19df3dc",
      "84b6912f1fff4b1183e57205fdaa8553",
      "cdf59213fb0342d3a8b4c96665d3c895",
      "311dc59a24244c54b5c4642973cbc492",
      "21e01d2765704369a139481b6c1eeec3",
      "4e632f04870b4447b139b20f287df3dc",
      "208ecb59ec8e4aee8192873d28741671",
      "4e95136a933247809bf4b34eecc2e174",
      "e0572b5b143449b9815424e012a6f763",
      "6a4a829a546b4c119fa153078dba8184",
      "b7d78a5348574184b44e9cebe2a7bb47",
      "9fd9ae5e8421430aaa9454796a81f7dd",
      "7dfe859f0d434184800730f461d3913a",
      "66cf40968b214c0aabc5e1321c2c7a89",
      "a59f67ec2a65463e9dac9c8f2b1095de",
      "87aa647f19624956bf650f93d64249a5",
      "867722b2e37c4570be74744152b0095a",
      "521f43e2ab5c48e2bee8daa0ef10a4ad",
      "0966adb3773248d0a34fd4412736a95d",
      "3fad11c848be4330ba544ad995c84bee",
      "aa68dc30fa4e451ab27767472ff5a96d",
      "fe496582f81846b29953a76df0f9b6ee",
      "e6b67b1c47384b4ab0a4f4b9901e61b0",
      "240527cb4c844fbfb7ba56957dcdbc5a",
      "d39ef216a0174d9095bc2d542c1b9b60",
      "145c5fd6625a43db8e917ae14d79725b",
      "3ddcad8ec2214962ba5e25f8f699a7bd",
      "52180db7f8fd421c8f38972df4e1f066",
      "1bcc4967e10a40e7808a6941fa12cf1b",
      "5453f42e194640f289b5eda1f7fcf5e3",
      "5e93c64de3e141dea554c110dc815652",
      "9dee466b242449e48d0526681306ae09",
      "8d9e3d50dcee4e08ae0f5f7e53e967c2",
      "2310a0472aa24dbd8b315cb0739eef9f",
      "d696c6fce36840d2b4e4ef89c09b4436",
      "aaa0f302473f47c6863fda788e46d5a1",
      "28c167b07ddd434eb43b42b7bd2af774",
      "adbafca178bf4f40909a739ccd547d3e",
      "0a53fe8efc16474795cd7150e7b1480a",
      "8a3d29f2662e4091bf1c960c491a6179",
      "64a1c21c38554e6c9aff594a10994c62",
      "92e0fd915127452dac2501da286b553d",
      "7c3b066aa6354d8ba60a95447d2b69b5",
      "6e33a9d8819f4f28b09d74ce5ffcba13",
      "188a5b96e2334412bea5c8161ecb1dd2",
      "cccde02e446445dc80c91e86eb475cdb",
      "e9b6a557d8fa416b89da78b87b297d85",
      "2699277cfe0b4f63aa466f63029cbb3d",
      "6423739bc90249a69e42f31f018dd2c7",
      "31c52eeff84146d4a887d30be6820004",
      "33aec7fa2f704b6f97aeb4e46e319aea",
      "cba2c9c0f3a0494ea98c717840efa0a0",
      "440883cdb27b4c95b56fe9a55592f50f",
      "cb864bf1f9544afeb855df924b304cc2",
      "ca376aa7d3434d8e9aae9b3668c332ac",
      "357246b9de9e49f2b4e75f6200cdf9b7",
      "562a293888dc459596def1d74fb1081d",
      "b282e0c52445443183664846abd0514a",
      "4e599429425e43ff8f86c4d1ad4ad8a5",
      "d39006cab8094382838640db52088641",
      "5c2bdcc3539849229cfcf43153f39338",
      "794555b7ca4f44998ef2e69ad8082de1",
      "c00242648c13415db39ddca3774bc48c",
      "35fd0c4b743c4da3bb24d7803f568804",
      "8e77fa07fd6c4b8ab478b3d86920d8a9",
      "e2091f0b8a864a5ea8537fdc945362ef",
      "f9972e297f434ca1ba1746cb4c984248",
      "5a7a6c193ed94502917c62b9a2994d98",
      "0b5e100c288d4f05b86b6daa590ec2ff",
      "866c9f5da73c40148a769a2d3cfac87e",
      "d4134ce3e3da4ae490fe78b97342d494",
      "e2e5e7cfbea94fae8ff8145aa6ab16ae",
      "c4b40f73923947b1b2333188abfa14b1",
      "a27e868675f54386a819148bd2dbea9d",
      "35d87bb3c11b471185df84458a9495f0",
      "d896b866026145aba3727a68c8c4ce4d",
      "59df3111b64047a3b6d0f162f0d8e6e6",
      "f0c112550adc4acabd2f6c30e49e141a",
      "a5a3d1874907443d9ec2977608fe641f",
      "1c8538cad6164f029dbb80dbe490fa47",
      "61327b076251475999acf9b1687b519d",
      "0eb06aaa55ee465b9ceedc7ad5b71bed",
      "5f4d97e131d043eba48c8ad25416b05b",
      "7baefaec856e4d5b99d20efbf69f46d3",
      "d76b729df9e741b3a0b46e5e751ce924",
      "e84a93befdf64de98702d3efd274aef8",
      "57aeb26402ca4f5380bdadf4daef9af1",
      "b5cf7be1ae5c4a5fa318c920456f4899",
      "509a3236307949308219bc39f3749952",
      "8d9a5d2b20b7406ab634d7175ce01fb8",
      "d7c519a9d3a5415f817240db5ea542a4",
      "6006d17ea9e2469784c10640ae50bd03",
      "f9d3c8154e7b49968078a8aa9273adb2",
      "2c9ac1bd4b8f489c9a2e3ff539d3a40c",
      "f2d37d790ae74a058eb75b8c46fef8e5",
      "4b0654e3387f4a36a7d691f4873e6b4e",
      "d1860293909c4c9fa60a5413e9627ee7",
      "e18fd602ce214921ac370defbbb59ec7",
      "c9aafafe02bd4d4bb078ab9803c1488b",
      "8b202733a9304a6e84cd20ac707daeca",
      "bcc40cdc1e04453da583b2ad7c2bbbe8"
     ]
    },
    "id": "7078423a",
    "outputId": "ec5a3236-b2d5-462a-8ed4-71571e202d5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a814d51fb3c416e944c314699a67013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ca27ccc009460bb8e8bbd7854d66ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ce52d9ed8349f39c9d637ca87e812e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f17714b31ea42adb0aac19a2619741d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8200a63fc44cca8828903b86675e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f6e822315a49cbb12d004e15d0992a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf59213fb0342d3a8b4c96665d3c895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cf40968b214c0aabc5e1321c2c7a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39ef216a0174d9095bc2d542c1b9b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa0f302473f47c6863fda788e46d5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b6a557d8fa416b89da78b87b297d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b282e0c52445443183664846abd0514a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5e100c288d4f05b86b6daa590ec2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8538cad6164f029dbb80dbe490fa47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c519a9d3a5415f817240db5ea542a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-4066916566.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.339543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.324923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.36646609497070315, metrics={'train_runtime': 254.7903, 'train_samples_per_second': 7.85, 'train_steps_per_second': 0.981, 'total_flos': 526222110720000.0, 'train_loss': 0.36646609497070315, 'epoch': 2.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Define smaller subsets for faster training and evaluation\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Training configurations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",      # Where to save checkpoints\n",
    "    eval_strategy=\"epoch\", # Evaluate every epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\" # Disable logging to Weights & Biases\n",
    ")\n",
    "\n",
    "# Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "f07187bc",
    "outputId": "434253da-a528-422d-a248-954053cf7ecb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['Positive', 'Negative']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# Create a simple Dataset from the tokenized inputs\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs):\n",
    "        self.tokenized_inputs = tokenized_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_inputs[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: self.tokenized_inputs[key][idx] for key in self.tokenized_inputs}\n",
    "\n",
    "# Example prediction\n",
    "text_to_predict = [\"This movie was fantastic!\", \"I really did not like this film.\"]\n",
    "\n",
    "# Tokenize the new text\n",
    "inputs = tokenizer(text_to_predict, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Create a PredictionDataset\n",
    "prediction_dataset = PredictionDataset(inputs)\n",
    "\n",
    "# Make predictions\n",
    "outputs = trainer.predict(prediction_dataset)\n",
    "\n",
    "# Get the predicted labels (0 for negative, 1 for positive)\n",
    "# The outputs are logits, so we need to apply a softmax and get the argmax\n",
    "predictions = np.argmax(outputs.predictions, axis=1)\n",
    "\n",
    "# Map predictions to labels\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "predicted_labels = [label_map[p] for p in predictions]\n",
    "\n",
    "print(\"Predictions:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc6a7539"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "78cdaf0c",
    "outputId": "3d4335d1-0a48-4ede-b9c2-1cfaeb9ddc9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3718284683.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1179' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1179/3125 15:17 < 25:16, 1.28 it/s, Epoch 0.38/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# --- 1. Define Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",            # where to save checkpoints\n",
    "    eval_strategy=\"epoch\",       # evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1, # Reduced number of epochs\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\" # Disable logging to Weights & Biases\n",
    ")\n",
    "\n",
    "# --- 2. Define Metric ---\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# --- 3. Create Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,                         # your AutoModelForSequenceClassification\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# --- 4. Start Training ---\n",
    "trainer.train()\n",
    "\n",
    "# --- 5. Evaluate ---\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
